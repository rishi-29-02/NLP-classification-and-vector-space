{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis Logistic Regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V-RVOniYAoP"
      },
      "source": [
        "# Natural Language Processing with classification and vector spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haSswMGebY87"
      },
      "source": [
        "# Sentiment Analysis using Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXrG3vSxbdUT"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy0f0If6bnvM",
        "outputId": "41654efc-2bf7-4fa4-afa4-b03e95727179"
      },
      "source": [
        "nltk.download('twitter_samples')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgBMOKnzbto4",
        "outputId": "b32ad761-7def-416f-d2b5-a153cbb103c3"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlCWRC_LdJ0m"
      },
      "source": [
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZELX8b7meBSI"
      },
      "source": [
        "Look at the raw texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwOguYUudY_Y",
        "outputId": "3abf0b68-24d1-4ba9-9933-60adc19a8ef9"
      },
      "source": [
        "print('The positive tweets : ')\n",
        "print('\\033[92m')\n",
        "for i in range(5):\n",
        "  print(all_positive_tweets[i])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The positive tweets : \n",
            "\u001b[92m\n",
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
            "@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
            "@97sides CONGRATS :)\n",
            "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxpRLUaAd5Fl",
        "outputId": "ab95ba2f-a7a6-4b15-d505-ba771123bb47"
      },
      "source": [
        "print('The negative tweets : ')\n",
        "print('\\033[91m')\n",
        "for i in range(5):\n",
        "  print(all_negative_tweets[i])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The negative tweets : \n",
            "\u001b[91m\n",
            "hopeless for tmr :(\n",
            "Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\n",
            "@Hegelbon That heart sliding into the waste basket. :(\n",
            "“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\n",
            "\n",
            "Me too\n",
            "Dang starting next week I have \"work\" :(\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xX08nAQbxck"
      },
      "source": [
        "## Building helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9rdwsyxcbyj"
      },
      "source": [
        "Process Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx9VnJipcA-R"
      },
      "source": [
        "import re  # Library for regular opeartions expressions\n",
        "import string\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "\n",
        "def process_tweet(tweet):\n",
        "  \n",
        "  stemmer = PorterStemmer()\n",
        "  stopwords_english = stopwords.words('english')\n",
        "\n",
        "  # remove the stock market tickers like $GE\n",
        "  tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "\n",
        "  # remove the old style retweet text RT\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "  # remove the hyperlinks\n",
        "  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "\n",
        "  # removing # symbol\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "  tweets_clean = []\n",
        "\n",
        "  for word in tweet_tokens:\n",
        "    if (word not in stopwords_english and\n",
        "        word not in string.punctuation):\n",
        "      stem_word = stemmer.stem(word)  #stemming\n",
        "      tweets_clean.append(stem_word)\n",
        "  \n",
        "  return tweets_clean"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw-d0En5h2Qv"
      },
      "source": [
        "Build Freqs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oArongvqiTaX"
      },
      "source": [
        "def build_freqs(tweets, ys):\n",
        "  yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "  freqs = {}\n",
        "  for y, tweet in zip(yslist, tweets):\n",
        "    for word in process_tweet(tweet):\n",
        "      pair = (word, y)\n",
        "      if pair in freqs:\n",
        "        freqs[pair]+=1\n",
        "      else:\n",
        "        freqs[pair] =1\n",
        "  return freqs"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpzM60XmlKZA"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBSdKR4Hlw8A"
      },
      "source": [
        "train_pos = all_positive_tweets[:4000]\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1QDOg9pmLYz"
      },
      "source": [
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJzMN55jmymw",
        "outputId": "73f722b9-2505-4fc1-d1c6-887e0ce73f23"
      },
      "source": [
        "print('Shape of train_y : ', train_y.shape)\n",
        "print('Shape of test_y : ', test_y.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train_y :  (8000, 1)\n",
            "Shape of test_y :  (2000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfxYfSJZnXnK",
        "outputId": "d9105aa8-baa4-4dab-9f52-56768c169715"
      },
      "source": [
        "# create a frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "\n",
        "print('Type of freqs : ', type(freqs))\n",
        "print('Length of freqs : ', len(freqs))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of freqs :  <class 'dict'>\n",
            "Length of freqs :  11346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V9jhFA1nyo4"
      },
      "source": [
        "**Process tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naDr4-QwoCkM",
        "outputId": "362d4c58-a0df-446a-cac0-541960b16507"
      },
      "source": [
        "print('This is an example of tweet before processing : \\n\\n', train_x[0])\n",
        "print('\\nAfter process the tweets :\\n\\n', process_tweet(train_x[0]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is an example of tweet before processing : \n",
            "\n",
            " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "After process the tweets :\n",
            "\n",
            " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc82y5ZQoW6d"
      },
      "source": [
        "## Part 1. Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inyatVG5oyUa"
      },
      "source": [
        "**Part 1.1 : Sigmoid**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_laxpv6Mo60H"
      },
      "source": [
        "def sigmoid(z):\n",
        "  h = 1/(1+np.exp(-z))\n",
        "  return h"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HjVlM4CptlQ"
      },
      "source": [
        "**Part 1.2 : Cost Function and Gradient**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8lCxI8VscWE"
      },
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "  m = x.shape[0]\n",
        "  for i in range(num_iters):\n",
        "    z = np.dot(x, theta)\n",
        "    h = sigmoid(z)\n",
        "\n",
        "    # cost function\n",
        "    J = (-1/m) * (np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h)))\n",
        "\n",
        "    # update the weights theta\n",
        "    theta = theta - (alpha/m)* (np.dot(x.T, h-y))\n",
        "\n",
        "  J = float(J)\n",
        "  return J, theta"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A1KdB5Hyjd3"
      },
      "source": [
        "## Part 2 : Extracting the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBwkfE8O0BG4"
      },
      "source": [
        "def extract_features(tweet, freqs):\n",
        "\n",
        "  # Process the tweet\n",
        "  word_l = process_tweet(tweet)\n",
        "\n",
        "  x = np.zeros((1, 3))\n",
        "\n",
        "  # bias term is set to 1\n",
        "  x[0, 0] = 1\n",
        "\n",
        "  for word in word_l:\n",
        "    x[0, 1] += freqs.get((word, 1), 0)\n",
        "    x[0, 2] += freqs.get((word, 0), 0)\n",
        "\n",
        "  assert(x.shape==(1, 3))\n",
        "  return x"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iONpsrc205rT"
      },
      "source": [
        "##Part 3 : Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N60H62a3h48",
        "outputId": "645b32d9-ac83-4ec1-e942-20b96a66ed02"
      },
      "source": [
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "  X[i, :] = extract_features(train_x[i], freqs)\n",
        "\n",
        "# training levels corrosponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Applying Gradient Descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cost after training is 0.24216529.\n",
            "The resulting vector of weights is [7e-08, 0.0005239, -0.00055517]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YweCPB943I8"
      },
      "source": [
        "## Part 4 : Test the Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36FaYCdE69SQ"
      },
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "\n",
        "  x = extract_features(tweet, freqs) \n",
        "\n",
        "  # make prediction\n",
        "  y_pred = sigmoid(np.dot(x, theta))\n",
        "\n",
        "  return y_pred"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmkYI-jo-BQL"
      },
      "source": [
        "def test_logistic_regression(test_x, test_y, feqs, theta):\n",
        "\n",
        "  # empty list for storing predictions\n",
        "  y_hat = []\n",
        "\n",
        "  for tweet in test_x:\n",
        "    y_pred = predict_tweet(tweet, freqs, theta)\n",
        "\n",
        "    if y_pred >0.5:\n",
        "      y_hat.append(1)\n",
        "    else:\n",
        "      y_hat.append(0)\n",
        "    \n",
        "  accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
        "  return accuracy "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okEwxy6n_zPJ",
        "outputId": "0df3f4a3-deaa-47b8-d201-d6185e8b6fb2"
      },
      "source": [
        "accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print('The accuracy of Logistic Regression is :', accuracy)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of Logistic Regression is : 0.995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD73n8wiCApF"
      },
      "source": [
        "##Part 5 : Predict our own tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogi3zbiXAJiC",
        "outputId": "10fa1794-86a3-4307-eae5-40531852c2d6"
      },
      "source": [
        "# Feel free to change the tweet below\n",
        "my_tweet = 'I am enjoying learning Natural Language Processing #NLP'\n",
        "print(process_tweet(my_tweet))\n",
        "\n",
        "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
        "\n",
        "if y_hat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else: \n",
        "    print('Negative sentiment')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['enjoy', 'learn', 'natur', 'languag', 'process', 'nlp']\n",
            "Positive sentiment\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}